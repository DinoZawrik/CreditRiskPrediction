[üá¨üáß English](README.md) | [üá∑üá∫ –†—É—Å—Å–∫–∏–π](README.ru.md)

# Credit Risk Prediction using Machine Learning

[![Project Status](https://img.shields.io/badge/Status-Complete-brightgreen.svg)](https://github.com/yourusername/yourrepository)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python Version](https://img.shields.io/badge/Python-3.7+-yellow.svg)](https://www.python.org/)
[![Libraries](https://img.shields.io/badge/Libraries-Pandas%2C%20Scikit--learn%2C%20XGBoost%2C%20CatBoost%2C%20LightGBM-orange.svg)](https://github.com/yourusername/yourrepository)

## Overview

This project focuses on building a machine learning model to predict credit risk, specifically the probability of a borrower experiencing serious delinquency (90 days past due or worse).  Accurate credit risk assessment is crucial for financial institutions to make informed lending decisions and manage risk effectively.

This project utilizes the "Give Me Some Credit" dataset from Kaggle to train and evaluate various classification models.

## Dataset

*   **Name:** Give Me Some Credit
*   **Source:** [Kaggle](https://www.kaggle.com/c/GiveMeSomeCredit)
*   **Description:**  This dataset contains personal financial data and credit history information for a large number of borrowers. The target variable `SeriousDlqin2yrs` indicates whether a borrower has experienced serious delinquency in the past two years.

## Features

The dataset includes the following features:

*   `SeriousDlqin2yrs`:  Target variable - Person experienced 90 days past due delinquency or worse (1) or not (0).
*   `RevolvingUtilizationOfUnsecuredLines`:  Utilization rate of unsecured lines of credit.
*   `age`: Borrower age in years.
*   `NumberOfTime30-59DaysPastDueNotWorse`: Number of times borrower has been 30-59 days past due but no worse in the last 2 years.
*   `DebtRatio`: Debt ratio.
*   `MonthlyIncome`: Monthly income.
*   `NumberOfOpenCreditLinesAndLoans`: Number of open credit lines and loans.
*   `NumberOfTimes90DaysLate`: Number of times borrower has been 90 days or more past due.
*   `NumberRealEstateLoansOrLines`: Number of real estate loans or lines.
*   `NumberOfTime60-89DaysPastDueNotWorse`: Number of times borrower has been 60-89 days past due but no worse in the last 2 years.
*   `NumberOfDependents`: Number of dependents.
*   **Engineered Features:** (List any new features you created, e.g., `IncomePerPerson`, `AgeGroup`, `PastDueCount`, etc.)

## Models Implemented

The following machine learning models were implemented and evaluated:

*   Logistic Regression
*   Random Forest
*   XGBoost
*   CatBoost
*   LightGBM

Hyperparameter tuning was performed using GridSearchCV and RandomizedSearchCV to optimize model performance. SMOTE (Synthetic Minority Over-sampling Technique) was used to address class imbalance in the dataset.

## Key Project Steps

1.  **Exploratory Data Analysis (EDA):**  Data exploration, visualization, and initial insights.
2.  **Data Preprocessing:** Handling missing values, outlier treatment, feature scaling.
3.  **Feature Engineering:** Creation of new features to improve model accuracy.
4.  **Model Training and Evaluation:** Training and evaluating various classification models using cross-validation and appropriate metrics.
5.  **Hyperparameter Tuning:** Optimizing model parameters for best performance.
6.  **Model Selection:** Choosing the best performing model based on evaluation metrics (primarily ROC AUC).
7.  **Submission File Generation:** Creating a submission file in the required format for Kaggle or similar platforms.

## Libraries Used

*   **pandas:** Data manipulation and analysis.
*   **numpy:** Numerical computing.
*   **matplotlib & seaborn:** Data visualization.
*   **scikit-learn (sklearn):** Machine learning algorithms, model evaluation, data preprocessing.
*   **imblearn:**  Handling imbalanced datasets (SMOTE).
*   **xgboost:**  Gradient boosting algorithm.
*   **catboost:** Gradient boosting algorithm.
*   **lightgbm:** Gradient boosting algorithm.
*   **optuna (optional):**  For Bayesian hyperparameter optimization (if used).

## Repository Files

*   `GiveMeSomeCredit-training.csv`: Training dataset.
*   `GiveMeSomeCredit-testing.csv`: Testing dataset.
*   `main.ipynb`: Jupyter Notebook containing the project code and analysis. (e.g., `CreditRiskPrediction.ipynb`)
*   `submission.csv`:  Example submission file generated by the best model.
*   `README.en.md`: English README file (this file).
*   `README.ru.md`: Russian README file.

## How to Run

1.  **Clone the repository**
2.  **Install required libraries:** 
    ```bash
    pip install pandas numpy matplotlib seaborn scikit-learn imblearn xgboost catboost lightgbm optuna
    ```
3.  **Run the Jupyter Notebook:**
    ```bash
    jupyter notebook main.ipynb
    ```
    Execute the notebook cells sequentially to reproduce the analysis, model training, and submission file generation.

## Results

The best performing model achieved a ROC AUC score of 0.85162 on the validation/test dataset.

For detailed results and analysis, please refer to the Jupyter Notebook.

## Further Improvements

*   **Advanced Feature Engineering:** Explore more complex feature interactions and domain-specific features.
*   **Ensemble Methods:**  Experiment with ensemble techniques like stacking or blending to combine predictions from multiple models.
*   **Hyperparameter Optimization:**  Utilize more advanced hyperparameter tuning methods like Bayesian Optimization for potentially better model parameters.
*   **Threshold Tuning:** Optimize the classification threshold to balance precision and recall based on specific business needs.
*   **Error Analysis:** Conduct a detailed error analysis to understand model weaknesses and areas for improvement.

